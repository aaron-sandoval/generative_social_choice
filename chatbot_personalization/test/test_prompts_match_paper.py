from ast import literal_eval
import unittest
import json

import pandas as pd

from chatbot_personalization.queries.query_chatbot_personalization import (
    ChatbotPersonalizationAgent,
    ChatbotPersonalizationGenerator,
)
from chatbot_personalization.utils.helper_functions import get_base_dir_path
import random
from string import Template


# The purpose of these tests is to make sure that the prompts generated by our code match the prompts from the logs
# from our experiment. There is one discrepancy: in our original logs, users' original Prolific IDs were included in
# the prompt, but in this published repo, to protect the privacy of our study participants, we've replaced these with
# user IDs of the form `generationX` or `validationX`.


class TestPromptsMatchPaper(unittest.TestCase):
    def test_prompts_match_paper_discriminative_validation_replication_fast(self):
        # These are the original logs from our discriminative
        # query validation experiment
        df_original = pd.read_csv(
            get_base_dir_path() / "data" / "validate_disc_query_logs.csv"
        )

        # Sample agent and heldout_q, representing one query from the original experiment
        df = pd.read_csv(get_base_dir_path() / "data/chatbot_personalization_data.csv")
        df = df[df["sample_type"] == "generation"]

        agent_id = random.choice(df.user_id.unique())
        survey_responses = df[df.user_id == agent_id]
        heldout_q_idx = random.choice(
            survey_responses[
                survey_responses["detailed_question_type"] == "rating statement"
            ].index
        )
        agent = ChatbotPersonalizationAgent(
            id=agent_id,
            survey_responses=survey_responses.drop(heldout_q_idx),
            summary=None,
        )
        prompt = Template(agent.prompt_template).safe_substitute(
            statement=survey_responses.loc[heldout_q_idx, "question_text_for_llm"]
        )

        # Assert our prompt appeared in original logs

        self.assertTrue(prompt in df_original["prompt"].values)

    def test_prompts_match_paper_discriminative_replication_fast(self):
        with open(
            get_base_dir_path()
            / "test/test_data/discriminative_query_prompts_from_paper.json",
            "r",
        ) as f:
            public_agent_prompts = json.load(f)
            df = pd.read_csv(
                get_base_dir_path() / "data/chatbot_personalization_data.csv"
            )
            df = df[df["sample_type"] == "generation"]
            agents = []
            for id in df.user_id.unique():
                agent = ChatbotPersonalizationAgent(
                    id=id,
                    survey_responses=df[df.user_id == id],
                    summary=None,  # don't need summaries for discriminative queries
                )
                agents.append(agent)
            agent_prompts = {agent.get_id(): agent.prompt_template for agent in agents}
            self.assertEqual(public_agent_prompts, agent_prompts)

    def test_prompts_match_paper_generative_replication_fast(self):
        with open(
            get_base_dir_path()
            / "test/test_data/generative_query_prompt_from_paper.txt",
            "r",
        ) as f:
            original_prompt = f.read()

            # Load all 100 agents from generation set
            df = pd.read_csv(
                get_base_dir_path() / "data/chatbot_personalization_data.csv"
            )
            df = df[df["sample_type"] == "generation"]
            agents = []
            agent_id_to_summary = (
                pd.read_csv(get_base_dir_path() / "data/user_summaries_generation.csv")
                .set_index("user_id")["summary"]
                .to_dict()
            )
            for id in df.user_id.unique():
                agent = ChatbotPersonalizationAgent(
                    id=id,
                    survey_responses=df[df.user_id == id],
                    summary=agent_id_to_summary[
                        id
                    ],  # don't need summaries for discriminative queries
                )
                agents.append(agent)

            # Create generator
            generator = ChatbotPersonalizationGenerator()
            # Make generative query on all agents (first step in paper)
            _, log = generator.generate(agents)

            # Complete both dicts with None string so that we can
            # compare them as dicts
            original_prompt += 'None"}]'
            prompt = log[0]["prompt"] + 'None"}]'

            # literal_eval doesn't like the \/ in the strings, so un-escape them
            prompt = prompt.replace("\\/", "/")
            original_prompt = original_prompt.replace("\\/", "/")

            # Convert both to dicts that map user_id to statement directly
            prompt_as_dict = (
                pd.DataFrame(literal_eval(prompt))
                .set_index("user_id")["statement"]
                .to_dict()
            )
            original_prompt_as_dict = (
                pd.DataFrame(literal_eval(original_prompt))
                .set_index("user_id")["statement"]
                .to_dict()
            )

            self.assertEqual(original_prompt_as_dict, prompt_as_dict)
